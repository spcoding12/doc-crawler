"""
Doc Crawler - é€šç”¨æŠ€æœ¯æ–‡æ¡£çˆ¬å–å·¥å…·

äº¤äº’å¼å‘½ä»¤è¡Œå…¥å£
"""

import sys
import time
from pathlib import Path
from urllib.parse import urlparse

# ç¡®ä¿UTF-8è¾“å‡º
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

from src.fetcher import fetch_with_requests, FetchError
from src.detector import detect_framework
from src.extractor import extract_content
from src.exporter import PageContent, export_content
from src.images import process_images
from src.frameworks import (
    DocusaurusAdapter,
    VuePressAdapter,
    MkDocsAdapter,
    GitBookAdapter,
    GenericAdapter,
)


# æ¡†æ¶é€‚é…å™¨æ˜ å°„
ADAPTERS = {
    'docusaurus': DocusaurusAdapter,
    'vuepress': VuePressAdapter,
    'mkdocs': MkDocsAdapter,
    'gitbook': GitBookAdapter,
    'generic': GenericAdapter,
    'unknown': GenericAdapter,
}


def print_banner():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     Doc Crawler v1.0                           â•‘
â•‘              é€šç”¨æŠ€æœ¯æ–‡æ¡£çˆ¬å–å·¥å…·                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def get_user_input(prompt: str, default: str = "") -> str:
    """è·å–ç”¨æˆ·è¾“å…¥"""
    if default:
        user_input = input(f"{prompt} [{default}]: ").strip()
        return user_input if user_input else default
    return input(f"{prompt}: ").strip()


def get_choice(prompt: str, options: list[str], default: int = 1) -> int:
    """è·å–ç”¨æˆ·é€‰æ‹©"""
    print(f"\n{prompt}")
    for i, opt in enumerate(options, 1):
        print(f"  {i}. {opt}")
    
    while True:
        choice = input(f"è¯·é€‰æ‹© [{default}]: ").strip()
        if not choice:
            return default
        try:
            num = int(choice)
            if 1 <= num <= len(options):
                return num
        except ValueError:
            pass
        print("  æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


def crawl_docs(
    start_url: str,
    output_format: str = "single",
    download_images: bool = True,
    output_dir: Path = Path("./output"),
) -> dict:
    """
    çˆ¬å–æ–‡æ¡£çš„ä¸»å‡½æ•°
    
    Args:
        start_url: èµ·å§‹URL
        output_format: è¾“å‡ºæ ¼å¼ (single/multiple/json)
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        dict: çˆ¬å–ç»“æœ
    """
    print(f"\nğŸ” æ­£åœ¨åˆ†æ: {start_url}")
    
    # 1. è·å–èµ·å§‹é¡µé¢
    try:
        result = fetch_with_requests(start_url)
    except FetchError as e:
        return {"success": False, "error": str(e)}
    
    # 2. æ£€æµ‹æ¡†æ¶
    framework = detect_framework(result.html)
    print(f"ğŸ“¦ æ£€æµ‹åˆ°æ¡†æ¶: {framework.name} (ç½®ä¿¡åº¦: {framework.confidence})")
    
    # æ ¹æ®URLè·¯å¾„ç”Ÿæˆå”¯ä¸€çš„ç«™ç‚¹ç›®å½•åï¼Œé¿å…åŒä¸€åŸŸåä¸‹ä¸åŒæ–‡æ¡£ç›¸äº’è¦†ç›–
    parsed_url = urlparse(start_url)
    site_name = parsed_url.netloc + parsed_url.path.rstrip('/')
    
    # 3. è·å–é€‚é…å™¨å¹¶è§£æä¾§è¾¹æ é“¾æ¥
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(result.html, 'lxml')
    
    adapter_class = ADAPTERS.get(framework.name, GenericAdapter)
    adapter = adapter_class()
    links = adapter.get_sidebar_links(soup, start_url)
    
    # å¦‚æœæ£€æµ‹åˆ°çš„é€‚é…å™¨æ‰¾ä¸åˆ°è¶³å¤Ÿé“¾æ¥ï¼Œå›é€€åˆ°GenericAdapter
    if len(links) < 5 and adapter_class != GenericAdapter:
        print(f"  âš ï¸ {framework.name}é€‚é…å™¨åªæ‰¾åˆ°{len(links)}ä¸ªé“¾æ¥ï¼Œå°è¯•é€šç”¨é€‚é…å™¨...")
        adapter = GenericAdapter()
        links = adapter.get_sidebar_links(soup, start_url)
    
    if not links:
        # å¦‚æœä»æ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œè‡³å°‘åŒ…å«å½“å‰é¡µé¢
        links = [{"url": start_url, "title": "Index", "level": 0}]
    
    print(f"ğŸ“„ å‘ç° {len(links)} ä¸ªé¡µé¢")
    
    # 5. çˆ¬å–æ‰€æœ‰é¡µé¢
    pages = []
    failed_pages = []
    
    for i, link in enumerate(links, 1):
        url = link['url']
        title = link['title']
        
        print(f"  [{i}/{len(links)}] {title[:40]}...", end=" ", flush=True)
        
        try:
            page_result = fetch_with_requests(url)
            content = extract_content(page_result.html, url)
            
            # å¤„ç†å›¾ç‰‡
            markdown = content.markdown
            if download_images and content.images:
                images_dir = output_dir / site_name
                markdown, img_results = process_images(
                    markdown, content.images, images_dir, download=True
                )
            
            pages.append(PageContent(
                url=url,
                title=content.title or title,
                markdown=markdown,
                images=content.images,
                level=link.get('level', 0),
                order=i,
            ))
            print("âœ…")
            
        except Exception as e:
            print(f"âŒ {str(e)[:30]}")
            failed_pages.append({"url": url, "error": str(e)})
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)
    
    print(f"\nâœ… æˆåŠŸçˆ¬å– {len(pages)} ä¸ªé¡µé¢")
    if failed_pages:
        print(f"âŒ å¤±è´¥ {len(failed_pages)} ä¸ªé¡µé¢")
    
    # 6. å¯¼å‡º
    export_result = export_content(
        pages, output_dir, format=output_format, site_name=site_name
    )
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {export_result['output_dir']}")
    print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {len(export_result['files'])} ä¸ª")
    
    return {
        "success": True,
        "pages_crawled": len(pages),
        "pages_failed": len(failed_pages),
        "failed_pages": failed_pages,
        "output": export_result,
    }


def main():
    """ä¸»å…¥å£"""
    print_banner()
    
    # 1. è·å–URL
    url = get_user_input("è¯·è¾“å…¥æ–‡æ¡£èµ·å§‹URL")
    if not url:
        print("âŒ URLä¸èƒ½ä¸ºç©º")
        return
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # 2. é€‰æ‹©è¾“å‡ºæ ¼å¼
    format_choice = get_choice(
        "é€‰æ‹©è¾“å‡ºæ ¼å¼:",
        [
            "å•ä¸ªMarkdownæ–‡ä»¶ (all.md) - é€‚åˆå¯¼å…¥NotebookLM",
            "å¤šä¸ªMarkdownæ–‡ä»¶ (æŒ‰ç« èŠ‚) - é€‚åˆæœ¬åœ°é˜…è¯»",
            "JSONæ–‡ä»¶ - é€‚åˆç¨‹åºå¤„ç†",
        ],
        default=1
    )
    format_map = {1: "single", 2: "multiple", 3: "json"}
    output_format = format_map[format_choice]
    
    # 3. æ˜¯å¦ä¸‹è½½å›¾ç‰‡
    img_choice = get_choice(
        "æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°?",
        ["æ˜¯ - ä¸‹è½½å›¾ç‰‡å¹¶æ›´æ–°é“¾æ¥", "å¦ - ä¿ç•™åŸå§‹URL"],
        default=1
    )
    download_images = (img_choice == 1)
    
    # 4. è¾“å‡ºç›®å½•
    output_dir = Path(get_user_input("è¾“å‡ºç›®å½•", "./output"))
    
    # 5. ç¡®è®¤
    print("\n" + "="*50)
    print("é…ç½®ç¡®è®¤:")
    print(f"  URL: {url}")
    print(f"  è¾“å‡ºæ ¼å¼: {output_format}")
    print(f"  ä¸‹è½½å›¾ç‰‡: {'æ˜¯' if download_images else 'å¦'}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print("="*50)
    
    confirm = input("\nå¼€å§‹çˆ¬å–? (Y/n): ").strip().lower()
    if confirm == 'n':
        print("å·²å–æ¶ˆ")
        return
    
    # 6. æ‰§è¡Œçˆ¬å–
    result = crawl_docs(
        start_url=url,
        output_format=output_format,
        download_images=download_images,
        output_dir=output_dir,
    )
    
    if result["success"]:
        print("\n" + "="*50)
        print("ğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"   æˆåŠŸ: {result['pages_crawled']} é¡µ")
        if result['pages_failed'] > 0:
            print(f"   å¤±è´¥: {result['pages_failed']} é¡µ")
        print("="*50)
    else:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {result.get('error')}")


if __name__ == "__main__":
    main()
